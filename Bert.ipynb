{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:30:13.108004Z",
          "iopub.status.busy": "2024-04-05T04:30:13.107632Z",
          "iopub.status.idle": "2024-04-05T04:30:23.486942Z",
          "shell.execute_reply": "2024-04-05T04:30:23.486087Z",
          "shell.execute_reply.started": "2024-04-05T04:30:13.107975Z"
        },
        "id": "kZ5KcbPq6kRd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "from torch.nn.parallel import DataParallel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:30:25.836615Z",
          "iopub.status.busy": "2024-04-05T04:30:25.835594Z",
          "iopub.status.idle": "2024-04-05T04:30:26.971963Z",
          "shell.execute_reply": "2024-04-05T04:30:26.970934Z",
          "shell.execute_reply.started": "2024-04-05T04:30:25.836585Z"
        },
        "id": "b2DxmuV96kRd",
        "outputId": "5a31396a-f032-431f-a043-83f76cefd5ac",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>i just feel really helpless and heavy hearted</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>i gave up my internship with the dmrg and am f...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                               text  label\n",
              "0           0      i just feel really helpless and heavy hearted      4\n",
              "1           1  ive enjoyed being able to slouch about relax a...      0\n",
              "2           2  i gave up my internship with the dmrg and am f...      4"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"text.csv\")\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6xHmlbP_426"
      },
      "source": [
        "# We need to traverse out dataset and try to find the distribution of each **classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:30:30.051117Z",
          "iopub.status.busy": "2024-04-05T04:30:30.050762Z",
          "iopub.status.idle": "2024-04-05T04:30:30.057238Z",
          "shell.execute_reply": "2024-04-05T04:30:30.056315Z",
          "shell.execute_reply.started": "2024-04-05T04:30:30.05109Z"
        },
        "id": "N8Wn2olP6kRe",
        "outputId": "cd59910d-68ff-4e98-8e3f-de9f2ab2ee41",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(416809, 3)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-02T16:44:14.51671Z",
          "iopub.status.busy": "2024-04-02T16:44:14.516438Z",
          "iopub.status.idle": "2024-04-02T16:44:14.538146Z",
          "shell.execute_reply": "2024-04-02T16:44:14.537133Z",
          "shell.execute_reply.started": "2024-04-02T16:44:14.516688Z"
        },
        "id": "Y5bgIhQ6CEql",
        "outputId": "54712eb5-ca66-4940-d695-94aa9532ecf6",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "1    141067\n",
              "0    121187\n",
              "3     57317\n",
              "4     47712\n",
              "2     34554\n",
              "5     14972\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.label.value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx52x2gGCOTu"
      },
      "source": [
        "# We can see the distribution of each class is unbalanced, such bias existing within data set may cause the prediction made by our model to proned to a certain class with more data points.Therefore, we need to eliminate such bias by balancing the number of each class passed into model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2024-04-05T04:30:46.760945Z",
          "iopub.status.busy": "2024-04-05T04:30:46.75995Z",
          "iopub.status.idle": "2024-04-05T04:30:46.855409Z",
          "shell.execute_reply": "2024-04-05T04:30:46.85423Z",
          "shell.execute_reply.started": "2024-04-05T04:30:46.760912Z"
        },
        "id": "qSSQEooL6kRf",
        "outputId": "b5e6421c-5ce3-4d28-9cdd-c9359f993c50",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_35/2166718454.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n"
          ]
        }
      ],
      "source": [
        "class balanced_data(Dataset):\n",
        "    def __init__(self, df, length=None):\n",
        "        if length is not None and length > df.shape[0]:\n",
        "            raise ValueError(\"Length parameter cannot be greater than the size of the dataset.\")\n",
        "        self.length = length if length is not None else len(df)\n",
        "        self.df = self.stratify(df)\n",
        "\n",
        "    def stratify(self, df):\n",
        "        min_count = df['label'].value_counts().min()\n",
        "        df = df.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
        "        return df.sample(self.length)\n",
        "    def get_all(self):\n",
        "        return self.df\n",
        "\n",
        "df_balanced = balanced_data(df,25000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:30:50.048257Z",
          "iopub.status.busy": "2024-04-05T04:30:50.047901Z",
          "iopub.status.idle": "2024-04-05T04:30:50.052589Z",
          "shell.execute_reply": "2024-04-05T04:30:50.051521Z",
          "shell.execute_reply.started": "2024-04-05T04:30:50.048231Z"
        },
        "id": "RV7XHqhk6kRg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_balanced =df_balanced.get_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaMsRxvFDz2-"
      },
      "source": [
        "# We can manually adjust the number of data points visible to the model by changing the second parameter of class balanced_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:30:53.266318Z",
          "iopub.status.busy": "2024-04-05T04:30:53.265502Z",
          "iopub.status.idle": "2024-04-05T04:30:53.273773Z",
          "shell.execute_reply": "2024-04-05T04:30:53.272797Z",
          "shell.execute_reply.started": "2024-04-05T04:30:53.266285Z"
        },
        "id": "Gtn7ALtC6kRg",
        "outputId": "7dae3b8b-1ab7-4653-fd74-3348b868e012",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "1    4233\n",
              "3    4203\n",
              "5    4183\n",
              "0    4181\n",
              "4    4114\n",
              "2    4086\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_balanced.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDC-HDx5EglM"
      },
      "source": [
        "# Because the english texts stored in dataframe are hard to understand and interpret for our model,tokenization is needed for converting text sequence into words and further encoding is also applied for the convertion of vectors.\n",
        "# After proper pre-processing, the dataset and dataloader were created for loading data into model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0d02f26345fb40569bf1eab1d3071964",
            "fc9ac990ef144d0ebca8f8d58ec32429",
            "d7d2136a87084ed481cfd3bffc87c8cb",
            "648bca39dc214e498d1b2a915f918cec",
            "8c405b7747f74e9382797416e3428557"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-04-04T23:17:17.107839Z",
          "iopub.status.busy": "2024-04-04T23:17:17.107556Z",
          "iopub.status.idle": "2024-04-04T23:17:25.609049Z",
          "shell.execute_reply": "2024-04-04T23:17:25.607944Z",
          "shell.execute_reply.started": "2024-04-04T23:17:17.107814Z"
        },
        "id": "NXQ9u_LY6kRg",
        "outputId": "1b0ab145-f238-4493-e64b-52b45cb0a9fd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to tokenize texts\n",
        "def tokenize_texts(tokenizer, texts, max_length=128):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Function to create a dataset from tokenized texts and labels\n",
        "def create_dataset(encodings, labels):\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.float64)\n",
        "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels_tensor)\n",
        "    return dataset\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df_balanced[\"text\"], df_balanced[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize texts\n",
        "train_encodings = tokenize_texts(tokenizer, train_texts.tolist())\n",
        "val_encodings = tokenize_texts(tokenizer, val_texts.tolist())\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = create_dataset(train_encodings, np.array(train_labels))\n",
        "val_dataset = create_dataset(val_encodings, np.array(val_labels))\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-04T23:17:25.610925Z",
          "iopub.status.busy": "2024-04-04T23:17:25.610377Z",
          "iopub.status.idle": "2024-04-04T23:17:25.61742Z",
          "shell.execute_reply": "2024-04-04T23:17:25.616446Z",
          "shell.execute_reply.started": "2024-04-04T23:17:25.610893Z"
        },
        "id": "X21bN9wg6kRg",
        "outputId": "5f842353-40eb-4672-86a7-877e472c84cd",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zMXO0HeFM5z"
      },
      "source": [
        "# We consider to construct a nn with Bert and a layer of fully connected layer with pooling. We will implement backpropagation for optimization and using adam for optimzer and cross entropy loss as loss function to minimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-04T23:17:25.622162Z",
          "iopub.status.busy": "2024-04-04T23:17:25.621887Z",
          "iopub.status.idle": "2024-04-04T23:51:47.110078Z",
          "shell.execute_reply": "2024-04-04T23:51:47.109182Z",
          "shell.execute_reply.started": "2024-04-04T23:17:25.622139Z"
        },
        "id": "fJNyHPue6kRh",
        "outputId": "295d186c-734e-41f0-a992-69a4691f7acf",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] loss: 0.762 |  Val loss: 0.202 | acc: 0.72  | Val acc: 0.94 | \n",
            "[2] loss: 0.190 |  Val loss: 0.155 | acc: 0.94  | Val acc: 0.95 | \n",
            "[3] loss: 0.135 |  Val loss: 0.137 | acc: 0.95  | Val acc: 0.95 | \n",
            "[4] loss: 0.115 |  Val loss: 0.133 | acc: 0.95  | Val acc: 0.96 | \n",
            "[5] loss: 0.101 |  Val loss: 0.146 | acc: 0.96  | Val acc: 0.95 | \n",
            "[6] loss: 0.093 |  Val loss: 0.143 | acc: 0.96  | Val acc: 0.95 | \n",
            "[7] loss: 0.086 |  Val loss: 0.152 | acc: 0.96  | Val acc: 0.95 | \n",
            "[8] loss: 0.076 |  Val loss: 0.171 | acc: 0.97  | Val acc: 0.95 | \n",
            "[9] loss: 0.064 |  Val loss: 0.177 | acc: 0.97  | Val acc: 0.94 | \n",
            "[10] loss: 0.059 |  Val loss: 0.196 | acc: 0.98  | Val acc: 0.95 | \n"
          ]
        }
      ],
      "source": [
        "class PlainBert_with_fcl(nn.Module):\n",
        "    def __init__(self, transformer_model, num_classes):\n",
        "        super(PlainBert_with_fcl, self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = output.pooler_output\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "num_classes =6\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PlainBert_with_fcl(model, num_classes)\n",
        "model = DataParallel(model)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        labels = labels.to(device).long()\n",
        "        outputs = outputs.float()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    train_accuracy = total_correct / len(train_dataset)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_correct = 0\n",
        "    val_predicted = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            outputs = outputs.float()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val_correct += (predicted == labels).sum().item()\n",
        "            val_predicted.extend(predicted.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = total_val_correct / len(val_dataset)\n",
        "\n",
        "    print('[%d] loss: %.3f |  Val loss: %.3f | acc: %.2f  | Val acc: %.2f | ' % (epoch + 1, train_loss, val_loss, train_accuracy, val_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwIOzCk4GHAc"
      },
      "source": [
        "# Using other form of evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-03T18:48:33.773703Z",
          "iopub.status.busy": "2024-04-03T18:48:33.773368Z",
          "iopub.status.idle": "2024-04-03T18:48:33.790754Z",
          "shell.execute_reply": "2024-04-03T18:48:33.789911Z",
          "shell.execute_reply.started": "2024-04-03T18:48:33.773644Z"
        },
        "id": "6EWkcr2nBOR9",
        "outputId": "2f48daef-1f2d-4a0a-993e-a75816a7be8f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.97      0.95      0.96       839\n",
            "     Class 1       0.98      0.89      0.94       811\n",
            "     Class 2       0.92      0.99      0.95       804\n",
            "     Class 3       0.96      0.95      0.95       869\n",
            "     Class 4       0.94      0.91      0.92       820\n",
            "     Class 5       0.92      1.00      0.96       857\n",
            "\n",
            "    accuracy                           0.95      5000\n",
            "   macro avg       0.95      0.95      0.95      5000\n",
            "weighted avg       0.95      0.95      0.95      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "val_predicted = np.array(val_predicted)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "report = classification_report(val_labels, val_predicted, target_names=[f'Class {i}' for i in range(num_classes)])\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmt594tmGMKE"
      },
      "source": [
        "# Test our model performance on all data points(420,000) instead of our sliced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-04T23:51:47.111654Z",
          "iopub.status.busy": "2024-04-04T23:51:47.11133Z",
          "iopub.status.idle": "2024-04-04T23:53:10.43357Z",
          "shell.execute_reply": "2024-04-04T23:53:10.432637Z",
          "shell.execute_reply.started": "2024-04-04T23:51:47.111628Z"
        },
        "id": "3uBqxijW6kRh",
        "outputId": "8461ef9e-c8e5-44a4-b758-8c38189cd2f2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing accuracy is: 97 %\n"
          ]
        }
      ],
      "source": [
        "test_encodings = tokenize_texts(tokenizer, df_balanced[\"text\"].tolist())\n",
        "test_dataset = create_dataset(test_encodings, np.array(df_balanced[\"label\"]))\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "def test_model(model):\n",
        "    total_test_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            labels = labels.to(device).long()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test_correct += (predicted == labels).sum().item()\n",
        "    print('Testing accuracy is: %d %%' % (100 * total_test_correct / len(test_dataset)))\n",
        "test_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S291qrvGXUE"
      },
      "source": [
        "We will then try to do some hyperparameter tuning by comparing the performance for big(32) and small(4) batch size, large(1e-5) and small(0.0001) learning rate.Therefore, we have 4 combinations of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-04T23:53:10.435181Z",
          "iopub.status.busy": "2024-04-04T23:53:10.434819Z",
          "iopub.status.idle": "2024-04-04T23:53:10.440293Z",
          "shell.execute_reply": "2024-04-04T23:53:10.439426Z",
          "shell.execute_reply.started": "2024-04-04T23:53:10.435149Z"
        },
        "id": "JjJx2_k66kRh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "small_batch_trainloader=DataLoader(train_dataset, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-04T23:53:10.441582Z",
          "iopub.status.busy": "2024-04-04T23:53:10.441314Z",
          "iopub.status.idle": "2024-04-04T23:53:14.891419Z",
          "shell.execute_reply": "2024-04-04T23:53:14.890392Z",
          "shell.execute_reply.started": "2024-04-04T23:53:10.441549Z"
        },
        "id": "FivpaN9m6kRh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to tokenize texts\n",
        "def tokenize_texts(tokenizer, texts, max_length=128):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Function to create a dataset from tokenized texts and labels\n",
        "def create_dataset(encodings, labels):\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.float64)\n",
        "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels_tensor)\n",
        "    return dataset\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df_balanced[\"text\"], df_balanced[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize texts\n",
        "train_encodings = tokenize_texts(tokenizer, train_texts.tolist())\n",
        "val_encodings = tokenize_texts(tokenizer, val_texts.tolist())\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = create_dataset(train_encodings, np.array(train_labels))\n",
        "val_dataset = create_dataset(val_encodings, np.array(val_labels))\n",
        "\n",
        "# Initialize DataLoaders\n",
        "small_batch_trainloader=DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-04T23:53:14.893453Z",
          "iopub.status.busy": "2024-04-04T23:53:14.892871Z",
          "iopub.status.idle": "2024-04-05T02:00:59.85544Z",
          "shell.execute_reply": "2024-04-05T02:00:59.854522Z",
          "shell.execute_reply.started": "2024-04-04T23:53:14.893416Z"
        },
        "id": "ag356V4i6kRh",
        "outputId": "3bc484b2-c611-4ec4-c7a1-153b4f0ac1ee",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] loss: 3.164 |  Val loss: 0.151 | acc: 0.86  | Val acc: 0.95 | \n",
            "[2] loss: 1.142 |  Val loss: 0.129 | acc: 0.95  | Val acc: 0.95 | \n",
            "[3] loss: 0.905 |  Val loss: 0.130 | acc: 0.95  | Val acc: 0.96 | \n",
            "[4] loss: 0.827 |  Val loss: 0.137 | acc: 0.96  | Val acc: 0.96 | \n",
            "[5] loss: 0.781 |  Val loss: 0.152 | acc: 0.96  | Val acc: 0.95 | \n",
            "[6] loss: 0.723 |  Val loss: 0.161 | acc: 0.96  | Val acc: 0.96 | \n",
            "[7] loss: 0.624 |  Val loss: 0.183 | acc: 0.97  | Val acc: 0.95 | \n",
            "[8] loss: 0.532 |  Val loss: 0.192 | acc: 0.97  | Val acc: 0.94 | \n",
            "[9] loss: 0.420 |  Val loss: 0.206 | acc: 0.98  | Val acc: 0.94 | \n",
            "[10] loss: 0.354 |  Val loss: 0.256 | acc: 0.98  | Val acc: 0.94 | \n"
          ]
        }
      ],
      "source": [
        "num_classes =6\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_ids = [0, 1]\n",
        "model = PlainBert_with_fcl(model, num_classes)\n",
        "model = nn.DataParallel(model, device_ids=device_ids)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "\n",
        "    for batch in small_batch_trainloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        labels = labels.to(device).long()\n",
        "        outputs = outputs.float()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    train_accuracy = total_correct / len(train_dataset)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_correct = 0\n",
        "    val_predicted = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            outputs = outputs.float()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val_correct += (predicted == labels).sum().item()\n",
        "            val_predicted.extend(predicted.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = total_val_correct / len(val_dataset)\n",
        "\n",
        "    print('[%d] loss: %.3f |  Val loss: %.3f | acc: %.2f  | Val acc: %.2f | ' % (epoch + 1, train_loss, val_loss, train_accuracy, val_accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:56:56.953445Z",
          "iopub.status.busy": "2024-04-05T04:56:56.953074Z",
          "iopub.status.idle": "2024-04-05T04:57:01.440543Z",
          "shell.execute_reply": "2024-04-05T04:57:01.439458Z",
          "shell.execute_reply.started": "2024-04-05T04:56:56.953416Z"
        },
        "id": "INn11Xox6kRi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to tokenize texts\n",
        "def tokenize_texts(tokenizer, texts, max_length=128):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Function to create a dataset from tokenized texts and labels\n",
        "def create_dataset(encodings, labels):\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.float64)\n",
        "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels_tensor)\n",
        "    return dataset\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df_balanced[\"text\"], df_balanced[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize texts\n",
        "train_encodings = tokenize_texts(tokenizer, train_texts.tolist())\n",
        "val_encodings = tokenize_texts(tokenizer, val_texts.tolist())\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = create_dataset(train_encodings, np.array(train_labels))\n",
        "val_dataset = create_dataset(val_encodings, np.array(val_labels))\n",
        "\n",
        "# Initialize DataLoaders\n",
        "small_batch_trainloader=DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:57:03.572015Z",
          "iopub.status.busy": "2024-04-05T04:57:03.571309Z",
          "iopub.status.idle": "2024-04-05T06:02:02.243746Z",
          "shell.execute_reply": "2024-04-05T06:02:02.242783Z",
          "shell.execute_reply.started": "2024-04-05T04:57:03.571978Z"
        },
        "id": "TdkuLO906kRi",
        "outputId": "211c9213-6e93-40b2-ba50-08a11130e299",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] loss: 14.521 |  Val loss: 1.823 | acc: 0.17  | Val acc: 0.16 | \n",
            "[2] loss: 14.502 |  Val loss: 1.809 | acc: 0.17  | Val acc: 0.17 | \n",
            "[3] loss: 14.509 |  Val loss: 1.806 | acc: 0.17  | Val acc: 0.16 | \n",
            "[4] loss: 14.503 |  Val loss: 1.834 | acc: 0.17  | Val acc: 0.17 | \n",
            "[5] loss: 14.507 |  Val loss: 1.804 | acc: 0.16  | Val acc: 0.16 | \n"
          ]
        }
      ],
      "source": [
        "num_classes =6\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_ids = [0, 1]\n",
        "model = PlainBert_with_fcl(model, num_classes)\n",
        "model = nn.DataParallel(model, device_ids=device_ids)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "\n",
        "    for batch in small_batch_trainloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        labels = labels.to(device).long()\n",
        "        outputs = outputs.float()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    train_accuracy = total_correct / len(train_dataset)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_correct = 0\n",
        "    val_predicted = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            outputs = outputs.float()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val_correct += (predicted == labels).sum().item()\n",
        "            val_predicted.extend(predicted.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = total_val_correct / len(val_dataset)\n",
        "\n",
        "    print('[%d] loss: %.3f |  Val loss: %.3f | acc: %.2f  | Val acc: %.2f | ' % (epoch + 1, train_loss, val_loss, train_accuracy, val_accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:32:18.688494Z",
          "iopub.status.busy": "2024-04-05T04:32:18.68762Z",
          "iopub.status.idle": "2024-04-05T04:32:23.233249Z",
          "shell.execute_reply": "2024-04-05T04:32:23.232234Z",
          "shell.execute_reply.started": "2024-04-05T04:32:18.688465Z"
        },
        "id": "V8GBOkP36kRi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to tokenize texts\n",
        "def tokenize_texts(tokenizer, texts, max_length=128):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "# Function to create a dataset from tokenized texts and labels\n",
        "def create_dataset(encodings, labels):\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.float64)\n",
        "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels_tensor)\n",
        "    return dataset\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df_balanced[\"text\"], df_balanced[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize texts\n",
        "train_encodings = tokenize_texts(tokenizer, train_texts.tolist())\n",
        "val_encodings = tokenize_texts(tokenizer, val_texts.tolist())\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = create_dataset(train_encodings, np.array(train_labels))\n",
        "val_dataset = create_dataset(val_encodings, np.array(val_labels))\n",
        "\n",
        "# Initialize DataLoaders\n",
        "train_dataloader=DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T04:32:39.973588Z",
          "iopub.status.busy": "2024-04-05T04:32:39.973167Z",
          "iopub.status.idle": "2024-04-05T04:50:48.571034Z",
          "shell.execute_reply": "2024-04-05T04:50:48.570074Z",
          "shell.execute_reply.started": "2024-04-05T04:32:39.973556Z"
        },
        "id": "erh-GkKQ6kRi",
        "outputId": "4287b70d-8d09-4906-f8bd-e23817855f03",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] loss: 0.405 |  Val loss: 0.157 | acc: 0.86  | Val acc: 0.94 | \n",
            "[2] loss: 0.158 |  Val loss: 0.147 | acc: 0.94  | Val acc: 0.94 | \n",
            "[3] loss: 0.167 |  Val loss: 0.176 | acc: 0.94  | Val acc: 0.94 | \n",
            "[4] loss: 0.137 |  Val loss: 0.151 | acc: 0.95  | Val acc: 0.94 | \n",
            "[5] loss: 0.133 |  Val loss: 0.173 | acc: 0.95  | Val acc: 0.94 | \n"
          ]
        }
      ],
      "source": [
        "num_classes =6\n",
        "\n",
        "class PlainBert_with_fcl(nn.Module):\n",
        "    def __init__(self, transformer_model, num_classes):\n",
        "        super(PlainBert_with_fcl, self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = output.pooler_output\n",
        "        logits = self.fc(pooled_output)\n",
        "        return logits\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_ids = [0, 1]\n",
        "model = PlainBert_with_fcl(model, num_classes)\n",
        "model = nn.DataParallel(model, device_ids=device_ids)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs=5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        labels = labels.to(device).long()\n",
        "        outputs = outputs.float()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    train_accuracy = total_correct / len(train_dataset)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_correct = 0\n",
        "    val_predicted = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            outputs = outputs.float()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val_correct += (predicted == labels).sum().item()\n",
        "            val_predicted.extend(predicted.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = total_val_correct / len(val_dataset)\n",
        "\n",
        "    print('[%d] loss: %.3f |  Val loss: %.3f | acc: %.2f  | Val acc: %.2f | ' % (epoch + 1, train_loss, val_loss, train_accuracy, val_accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu0V2B1WHAmC"
      },
      "source": [
        "# We can see three models shared similar performance whereas the combination of small batch size and large lr model showed a poor performance in term of low training and validation accuracy, this may caused by bad initialization of parameter and the difficulty for finding a correct way of updating weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-05T06:05:23.441827Z",
          "iopub.status.busy": "2024-04-05T06:05:23.441057Z"
        },
        "id": "BycuVAyi6kRj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "train_texts = train_texts.tolist()\n",
        "val_texts = val_texts.tolist()\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels,dtype=torch.float64))\n",
        "val_dataset = torch.utils.data.TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_labels,dtype=torch.float64))\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4PWuYY8Hpot"
      },
      "source": [
        "# We tried to add more layers to our model so that this model may better capture the inherent pattern within dataset. A convolutional later was added with kernel size of 3 and 256 filters will be learnt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nHezzQp6kRj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class PlainBert_with_fclWithConv(nn.Module):\n",
        "    def __init__(self, transformer_model, num_classes, kernel_size=3, num_filters=256):\n",
        "        super(PlainBert_with_fclWithConv, self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        self.conv = nn.Conv1d(in_channels=768, out_channels=num_filters, kernel_size=kernel_size, padding=1)\n",
        "        self.fc = nn.Linear(num_filters, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = output.pooler_output\n",
        "        pooled_output = pooled_output.unsqueeze(2)\n",
        "\n",
        "        conv_out = F.relu(self.conv(pooled_output))\n",
        "        pooled_conv_out, _ = torch.max(conv_out, dim=2)\n",
        "        logits = self.fc(pooled_conv_out)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-03T18:49:57.006704Z",
          "iopub.status.busy": "2024-04-03T18:49:57.006326Z",
          "iopub.status.idle": "2024-04-03T19:00:44.05979Z",
          "shell.execute_reply": "2024-04-03T19:00:44.05883Z",
          "shell.execute_reply.started": "2024-04-03T18:49:57.006673Z"
        },
        "id": "t31r4a8IBOR-",
        "outputId": "aca535af-c6b1-424e-fa19-9e10203ab7eb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] loss: 0.943 |  Val loss: 0.328 | acc: 0.66  | Val acc: 0.92 | \n",
            "[2] loss: 0.247 |  Val loss: 0.198 | acc: 0.93  | Val acc: 0.94 | \n",
            "[3] loss: 0.158 |  Val loss: 0.172 | acc: 0.95  | Val acc: 0.95 | \n"
          ]
        }
      ],
      "source": [
        "num_classes =6\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_ids = [0, 1]\n",
        "\n",
        "\n",
        "model = PlainBert_with_fclWithConv(model, num_classes)\n",
        "model = nn.DataParallel(model, device_ids=device_ids)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "num_epochs=3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        labels = labels.to(device).long()\n",
        "        outputs = outputs.float()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    train_accuracy = total_correct / len(train_dataset)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_correct = 0\n",
        "    val_predicted = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            outputs = outputs.float()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val_correct += (predicted == labels).sum().item()\n",
        "            val_predicted.extend(predicted.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_accuracy = total_val_correct / len(val_dataset)\n",
        "\n",
        "    print('[%d] loss: %.3f |  Val loss: %.3f | acc: %.2f  | Val acc: %.2f | ' % (epoch + 1, train_loss, val_loss, train_accuracy, val_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVEURCdKIXfD"
      },
      "source": [
        "# We may find a 2% increase in overall performance after convolution layer was added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-03T19:06:29.272366Z",
          "iopub.status.busy": "2024-04-03T19:06:29.271956Z",
          "iopub.status.idle": "2024-04-03T19:06:29.294279Z",
          "shell.execute_reply": "2024-04-03T19:06:29.292908Z",
          "shell.execute_reply.started": "2024-04-03T19:06:29.272332Z"
        },
        "id": "9HKHQoNoBOR-",
        "outputId": "6ed81a14-8afb-4698-f389-090e0390501d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.97      0.94      0.96       839\n",
            "     Class 1       0.99      0.89      0.93       811\n",
            "     Class 2       0.92      0.99      0.95       804\n",
            "     Class 3       0.96      0.96      0.96       869\n",
            "     Class 4       0.94      0.91      0.92       820\n",
            "     Class 5       0.92      1.00      0.96       857\n",
            "\n",
            "    accuracy                           0.95      5000\n",
            "   macro avg       0.95      0.95      0.95      5000\n",
            "weighted avg       0.95      0.95      0.95      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "val_predicted = np.array(val_predicted)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "report = classification_report(val_labels, val_predicted, target_names=[f'Class {i}' for i in range(num_classes)])\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 4403839,
          "sourceId": 7563141,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30674,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
